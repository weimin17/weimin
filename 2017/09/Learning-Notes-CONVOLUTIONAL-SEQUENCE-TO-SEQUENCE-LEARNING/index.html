<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/Coding.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Coding.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Coding.png">
  <link rel="mask-icon" href="/images/Coding.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Garamond:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://weimin17.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="| 论文(Arxiv)     | Blog Post(FAIR) |  Code(Github)   |">
<meta property="og:type" content="article">
<meta property="og:title" content="[Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING">
<meta property="og:url" content="http://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/index.html">
<meta property="og:site_name" content="A DL&#x2F;ML Learner">
<meta property="og:description" content="| 论文(Arxiv)     | Blog Post(FAIR) |  Code(Github)   |">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/1506120945555.png">
<meta property="og:image" content="https://pic3.zhimg.com/v2-0cd3e337910a1fca234065fb0896a75a_b.png">
<meta property="og:image" content="http://img.blog.csdn.net/20170419103042289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1Y2hvbmdl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170419110415306?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1Y2hvbmdl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://pic1.zhimg.com/v2-e304330c2dd915d7878641456d932684_b.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png">
<meta property="og:image" content="https://data-sci.info/wp-content/uploads/2017/05/fair.gif">
<meta property="article:published_time" content="2017-09-27T04:55:00.000Z">
<meta property="article:modified_time" content="2017-10-16T17:44:57.000Z">
<meta property="article:author" content="weimin">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/1506120945555.png">

<link rel="canonical" href="http://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>[Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING | A DL/ML Learner</title>
  
    <script>
      function sendPageView() {
        if (CONFIG.hostname !== location.hostname) return;
        var uid = localStorage.getItem('uid') || (Math.random() + '.' + Math.random());
        localStorage.setItem('uid', uid);
        navigator.sendBeacon('https://www.google-analytics.com/collect', new URLSearchParams({
          v  : 1,
          tid: 'UA-156005782-1',
          cid: uid,
          t  : 'pageview',
          dp : encodeURIComponent(location.pathname)
        }));
      }
      document.addEventListener('pjax:complete', sendPageView);
      sendPageView();
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A DL/ML Learner</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Train like a beast.</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives<span class="badge">33</span></a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar_idea.jpeg">
      <meta itemprop="name" content="weimin">
      <meta itemprop="description" content="DL/ML Blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A DL/ML Learner">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-09-27 00:55:00" itemprop="dateCreated datePublished" datetime="2017-09-27T00:55:00-04:00">2017-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2017-10-16 13:44:57" itemprop="dateModified" datetime="2017-10-16T13:44:57-04:00">2017-10-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning-Notes/" itemprop="url" rel="index">
                    <span itemprop="name">Learning Notes</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>| 论文(<a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">Arxiv</a>)     | Blog Post(<a href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/" target="_blank" rel="noopener">FAIR</a>) |  Code(<a href="https://github.com/facebookresearch/fairseq" target="_blank" rel="noopener">Github</a>)   |</p>
<a id="more"></a>
<p>來自FAIR團隊這幾天出的基於純CNN架構翻譯機:Convolutional Sequence to Sequence Learning。 其結果跟Google最新的方法比較後重點結果如下:</p>
<p>英翻法產生翻譯(WMT English-&gt;French)速度的表現:</p>
<p>在GPU產生翻譯的速度上，FAIR用K40(比較爛的GPU)是Google用K80(比較好的GPU)的9.3倍。<br>在CPU產生翻譯速度上，FAIR(48 cores)就已經是Google(88 cores)的17(per cpu core basis)倍，且還是Google自家用TPU的2.7倍。</p>
<p>綜合翻譯的表現:<br>WMT 2014 English-French:進步1.5BLEU<br>WMT 2014 English-German:進步0.5BLEU<br>WMT 2016 English-Romanian:進步1.8BLEU<br>看這速度跟效能，算是不小的突破。那這到底是怎麼做的呢? 下圖是這個模型的大架構:<br><img src="./1506120945555.png" alt="Alt text|center|450*600"></p>
<p>因為這一篇涉及的知識有點廣，所以三部分做引導和補充相關閱讀資料，這個Model的主要有三個部分:</p>
<p>Encoder-Decoder(左上和左下)<br>Residual Connect(沒標出來)<br>Multi-Hop Attention(中間那塊)</p>
<h2 id="第一部分-Encoder-Decoder-Gated-Linear-Unit-1"><a href="#第一部分-Encoder-Decoder-Gated-Linear-Unit-1" class="headerlink" title="第一部分: Encoder Decoder: Gated Linear Unit[1]"></a>第一部分: Encoder Decoder: Gated Linear Unit[1]</h2><p>[1]<a href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener">Daphine et.al “Language Modeling with Gated Convolutional Networks” 2016</a></p>
<p>目前语言模型主要基于RNN，这篇文章提出了一种新颖的语言模型，仿照LSTM中的门限机制，利用多层的CNN结构，每层CNN都加上一个输出门限。文章的主要贡献包括：</p>
<ol>
<li>提出一种新的门控机制</li>
<li>缓解梯度传播，降低梯度弥散等现象</li>
<li>相比LSTM，模型更加简单，收敛速度更快 </li>
</ol>
<p>模型的结构图如下所示： </p>
<p><img src="https://pic3.zhimg.com/v2-0cd3e337910a1fca234065fb0896a75a_b.png" alt="Alt text|center"></p>
<p>为了缓解梯度消失的问题，LSTM在RNN中引入门限机制：输入门，遗忘门和输出门。由于循环神经网络中每个时刻状态都不仅和输入相关，也和前一个时刻状态相关。所以在序列中无法并行化处理。这篇论文将门引入到CNN中，提出了一种新颖的模型：门限机制的卷积模型，并延续了CNN可以并行计算的优势。文中具体提出了GTU和GLU两种模型，并在实验阶段作了比较。这两个模型整体类似，主要是激活函数不一样。</p>
<p>其主要结构跟原始的CNN并无很大差异，只不过在卷积层引入了门控机制，将卷积层的输出变成了下面的公式，即一个没有非线性函数的卷积层输出*经过sigmod非线性激活函数的卷积层输出： </p>
<p><img src="http://img.blog.csdn.net/20170419103042289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1Y2hvbmdl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="Alt text|center"></p>
<p>其中W和V是不同的卷积核，卷积核宽度为k，输出通道数为n，b和c是偏置参数。上面公式中的后半部分，即有激活函数的卷积就是所谓的门控机制，其控制了X*W+b中哪些信息可以传入下一层。这里将其定义为Gated Linear Units (GLU).然后就可以将该模型进行堆叠，以捕获Long_Term memory。论文得出：GLU要比GTU好。</p>
<p><img src="http://img.blog.csdn.net/20170419110415306?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1Y2hvbmdl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="|center"></p>
<p>Dauphine在他的論文中有更清楚的圖和文字來說明如何用這樣的架構來達到更好的Language Model。簡單來說Gated Linear Unit會對同一個input word vector學出兩個representation，其中一個是用來決定哪些資訊要保留哪些要捨去。而且這個方法可以更有效讓Gradient Backpropogate。</p>
<p>通过下面两篇文章快速了解该论文:<br><a href="http://blog.csdn.net/liuchonge/article/details/70238350" target="_blank" rel="noopener">參考1</a>, <a href="https://zhuanlan.zhihu.com/p/24780258" target="_blank" rel="noopener">參考2</a></p>
<h2 id="第二部分-Residual-Connect-2"><a href="#第二部分-Residual-Connect-2" class="headerlink" title="第二部分: Residual Connect[2]"></a>第二部分: Residual Connect[2]</h2><p>[2]<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">He et.al. “Deep Residual Learning for Image Recognition”</a></p>
<p>這篇文章可以跟VGG文章一起看，VGG是暴力增加網絡層數，使得訓練效果達到很棒。但是需要很長的訓練時間。而本文提出的ResNet，構造了殘差方式，使得訓練速度、效果都達到很好的狀態。</p>
<p>一般来说越深的网络，越难被训练，Deep Residual Learning for Image Recognition中提出一种residual learning的框架，能够大大简化模型网络的训练时间，使得在可接受时间内，模型能够更深(152甚至尝试了1000)，该方法在ILSVRC2015上取得最好的成绩。「背景介紹：这篇文章是介绍Kaming He等人组成的微软亚洲研究院的队伍参加ILSVRC 2015比赛中所用的网络结构的论文，ResNet一出，简直是把ImageNet上的classification任务给做烂了，top5的正确率达到了惊人的96%以上，已经超过了人类的识别率，网络的层数达到了152层，甚至上千层（要知道2014年的VGGNet中的22层已经用very deep来形容了），目前人们应用的最广泛的也就是GoogLeNet、VGGNet和ResNet了。」</p>
<p>随着模型深度的增加，会产生以下问题：</p>
<ol>
<li>vanishing/exploding gradient，导致了训练十分难收敛，这类问题能够通过norimalized initialization 和intermediate normalization layers解决；</li>
<li>对合适的额深度模型再次增加层数，模型准确率会迅速下滑（不是overfit造成），training error和test error都会很高，相应的现象在CIFAR-10和ImageNet都有提及。</li>
</ol>
<p>为了解决因深度增加而产生的性能下降问题，作者提出下面一种结构来做residual learning：<br><img src="https://pic1.zhimg.com/v2-e304330c2dd915d7878641456d932684_b.png" alt="@ResNet結構|center|500*300"></p>
<p>这篇文章主要得改善就是对传统的卷积模型增加residual learning，通过残差优化来找到近似最优identity mappings。既然我们现在的网络训练方法难以训练处我们现在心中完美的模型（我们心中完美的就是新加层之后的表现很好，能够提高正确率），那么我们退一步，让我们的模型学到更容易学到的，稍差的模型。假设我们心目中的模型，新加的层本来要学到这样一个映射H(x)，现在我们降低要求，让他学到这样一个映射F(x):=H(x)-x，其中x为输入的数据，那么我们最初要学习的映射就是F(x)+x，我们假设学习F(x)比学H(x)要简单。这样一来我们训练一个这样的网络，每一层学习到F(x)的映射，然后我们人为的对在该层的输出加入原输入x。这其实就是ResNet的基本思路，思路简单，但作者能这么想，同时还有理论的支持（理论的支持在文章Related work部分)。理想的状态下问题就解决了，因为我们可以通過”shortcut connections”較容易實現F(x)+x。</p>
<p><a href="http://blog.csdn.net/lhanchao/article/details/64159046" target="_blank" rel="noopener">參考1</a>， <a href="https://zhuanlan.zhihu.com/p/23518167" target="_blank" rel="noopener">參考2</a>， <a href="http://jacobkong.github.io/posts/3085218970/" target="_blank" rel="noopener">參考3</a><br>這個方法其實是在2015 ResNet中提出的，目的是為了在訓練很深的神經網路時讓Gradient能夠更容易Backpropogate，其概念和實作可以參考這篇介紹。可以簡單想成是在很深的神經網路中，讓比較深的Gradient可以有一個捷徑直接往後丟以免再一步一步地傳遞過程中消失。</p>
<h2 id="第三部分-Seq-to-Seq"><a href="#第三部分-Seq-to-Seq" class="headerlink" title="第三部分: Seq to Seq"></a>第三部分: Seq to Seq</h2><p><a href="https://arxiv.org/pdf/1611.01576.pdf" target="_blank" rel="noopener">Quasi-Recurrent Neural Networks</a><br>将LSTM的门机制扩展成时间序列，关于Seq to Seq, 现在有RNN seq to seq，CNN Seq to Seq，以及介于两者之间的QRNN，此部分仅用于了解Seq的机制。</p>
<h2 id="第四部分-Multi-hop-Attention"><a href="#第四部分-Multi-hop-Attention" class="headerlink" title="第四部分: Multi-hop Attention"></a>第四部分: Multi-hop Attention</h2><p>首先要知道什么是attention。這裏兩篇博客，一篇宏觀介紹<a href="https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/" target="_blank" rel="noopener">Attention in Long Short-Term Memory Recurrent Neural Networks</a>，一篇從較細角度介紹<a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener">Attention and Memory in Deep Learning and NLP</a>。<br>Some points in these two blogs:</p>
<ol>
<li><strong>Why Attention</strong></li>
</ol>
<p>The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.<br>A limitation of the architecture is that it encodes the input sequence to a <strong>fixed length internal representation</strong>. This imposes limits on the length of input sequences that can be reasonably learned and results in <strong>worse performance</strong> for very <strong>long input sequences</strong>.</p>
<p>Attention is the idea of freeing the encoder-decoder architecture from <em>the fixed-length internal representation</em> (Maybe to overcome the disadvantages of <em>long input sequences</em> is better). This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence. Put another way, each item in the output sequence is conditional on selective items in the input sequence.</p>
<ol start="2">
<li><strong>If you look closely at the “Attention”</strong></li>
</ol>
<p>With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far.<br><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png" alt="@Neural Machine Translation by Jointly Learning to Align and Translate|center|300*300"></p>
<p>Here, The <code>y&#39;s</code> are our translated words produced by the decoder, and the <code>x&#39;s</code> are our source sentence words. The above illustration uses a bidirectional recurrent network, but that’s not important and you can just ignore the inverse direction. The important part is that each decoder output word y_t now depends on <strong>a weighted combination of all the input states</strong>, not just the last state. The a‘s are weights that define in how much of each input state should be considered for each output. So, if $a_{3,2}$ is a large number, this would mean that the decoder pays a lot of attention to the second state in the source sentence while producing the third word of the target sentence. The <code>a&#39;s</code> are typically normalized to sum to 1 (so they are a distribution over the input states).</p>
<p>A big advantage of attention is that it gives us the ability to interpret and visualize what the model is doing. For example, by visualizing the attention weight matrix a when a sentence is translated, we can understand how the model is translating:</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png" alt="@Visualization| center| 300*300"></p>
<ol start="3">
<li><strong>Disadvantages</strong></li>
</ol>
<p>This increases the computational burden of the model, but results in a more targeted and better-performing model.</p>
<p>在Convolutional Seq to Seq這篇文章中，用到了Multi-Step Attention。這裡必需先提一下Attention在機器翻譯中的應用，直覺上來說就是在翻譯的過程中，回去看一下待翻譯句子的前後文，更多細節可以參考[3]或是這篇文章中的Attention部分。Multi-Hop直覺上來說就是多重複幾次這樣的動作來達到更好的翻譯表現，這部分FAIR做了一個簡單的圖:</p>
<p><img src="https://data-sci.info/wp-content/uploads/2017/05/fair.gif" alt="Fair"></p>
<p>圖片來源: <a href="https://research.fb.com/category/facebook-ai-research-fair/" target="_blank" rel="noopener">FAIR</a></p>
<p>這三塊都有基本了解後這篇大概就能看懂八成了。不過這篇文還有其他細節像是Normalization,Initialization都有特別設計過，就留待讀者自行去探索了。</p>
<!--more-->

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/09/Deep-Learning-Recurrent-Neural-Networks-RNN/" rel="prev" title="[Deep Learning] Recurrent Neural Networks - RNN">
      <i class="fa fa-chevron-left"></i> [Deep Learning] Recurrent Neural Networks - RNN
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/09/Deep-Learning-Kernel/" rel="next" title="[Deep Learning] Kernel">
      [Deep Learning] Kernel <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#第一部分-Encoder-Decoder-Gated-Linear-Unit-1"><span class="nav-number">1.</span> <span class="nav-text">第一部分: Encoder Decoder: Gated Linear Unit[1]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二部分-Residual-Connect-2"><span class="nav-number">2.</span> <span class="nav-text">第二部分: Residual Connect[2]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第三部分-Seq-to-Seq"><span class="nav-number">3.</span> <span class="nav-text">第三部分: Seq to Seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第四部分-Multi-hop-Attention"><span class="nav-number">4.</span> <span class="nav-text">第四部分: Multi-hop Attention</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="weimin"
      src="/images/avatar_idea.jpeg">
  <p class="site-author-name" itemprop="name">weimin</p>
  <div class="site-description" itemprop="description">DL/ML Blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/weimin17" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;weimin17" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weimin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id: ,
      el: 'wpac-rating',
      color: 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>












  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '54ab485ae91fc57bf25e',
      clientSecret: '98349a9bcb969cadebace85cb2bdfd2ec7ab5184',
      repo: 'weimin17.github.io',
      owner: 'weimin17',
      admin: ['weimin17'],
      id: '9176b7964935fa435c86b30d4a384a73',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
