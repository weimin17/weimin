<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/Coding.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Coding.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Coding.png">
  <link rel="mask-icon" href="/images/Coding.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://weimin17.github.io').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="AttentionIn [Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Warehouse">
<meta property="og:url" content="http://weimin17.github.io/2017/11/Deep-Learning-Warehouse/index.html">
<meta property="og:site_name" content="A DL&#x2F;ML Learner">
<meta property="og:description" content="AttentionIn [Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png">
<meta property="article:published_time" content="2017-11-24T01:44:00.000Z">
<meta property="article:modified_time" content="2017-11-24T02:27:43.000Z">
<meta property="article:author" content="weimin">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png">

<link rel="canonical" href="http://weimin17.github.io/2017/11/Deep-Learning-Warehouse/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Deep Learning Warehouse | A DL/ML Learner</title>
  
    <script>
      function sendPageView() {
        if (CONFIG.hostname !== location.hostname) return;
        var uid = localStorage.getItem('uid') || (Math.random() + '.' + Math.random());
        localStorage.setItem('uid', uid);
        navigator.sendBeacon('https://www.google-analytics.com/collect', new URLSearchParams({
          v  : 1,
          tid: 'UA-156005782-1',
          cid: uid,
          t  : 'pageview',
          dp : encodeURIComponent(location.pathname)
        }));
      }
      document.addEventListener('pjax:complete', sendPageView);
      sendPageView();
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">A DL/ML Learner</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Train like a beast.</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives<span class="badge">33</span></a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://weimin17.github.io/2017/11/Deep-Learning-Warehouse/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar_idea.jpeg">
      <meta itemprop="name" content="weimin">
      <meta itemprop="description" content="DL/ML Blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="A DL/ML Learner">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning Warehouse
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2017-11-23 20:44:00 / Modified: 21:27:43" itemprop="dateCreated datePublished" datetime="2017-11-23T20:44:00-05:00">2017-11-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>In <a href="https://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/">[Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING</a></p>
<a id="more"></a>
<p>The forth part: Multi-hop Attention</p>
<p>The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains. A limitation of the architecture is that it encodes the input sequence to a <strong>fixed length internal representation</strong>. This imposes limits on the length of input sequences that can be reasonably learned and results in <strong>worse performance</strong> for very <strong>long input sequences</strong>.<br><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png" alt="@Neural Machine Translation by Jointly Learning to Align and Translate|center|300*300"></p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>An embedding maps an input representation, such as a word or sentence, into a vector. A popular type of embedding are word embeddings such as <a href="http://www.wildml.com/deep-learning-glossary/#word2vec" target="_blank" rel="noopener">word2vec</a> or <a href="http://www.wildml.com/deep-learning-glossary/#glove" target="_blank" rel="noopener">GloVe</a>. We can also embed sentences, paragraphs or images. For example, by mapping images and their textual descriptions into a common embedding space and minimizing the distance between them, we can match labels with images. Embeddings can be learned explicitly, such as in word2vec, or as part of a supervised task, such as Sentiment Analysis. Often, the input layer of a network is initialized with pre-trained embeddings, which are then fine-tuned to the task at hand.</p>
<p>Why embedding:</p>
<ol>
<li>在自然语言处理领域中，传统的做法是将词表示成离散的符号，例如将 [cat] 表示为 [Id537]，而 [dog] 表示为 [Id143]。这样做的缺点在于，没有提供足够的信息来体现词语之间的某种关联.</li>
<li>Maps the input representation into a vector. And fixed length of the vector.<h3 id="Word2vec-amp-GloVe"><a href="#Word2vec-amp-GloVe" class="headerlink" title="Word2vec &amp; GloVe"></a>Word2vec &amp; GloVe</h3><h4 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h4></li>
<li>Skip-Gram</li>
<li>Continuous Bag-of-Words</li>
</ol>
<p>直接看实例（Gensim包），这里用save_word2vec_format看输出的模型是什么样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">台湾 0.396402 1.611405 -0.291840 -0.951169 -0.109141 1.918246 0.215038 0.674539 2.335748 -0.757200 -0.290877 2.198100 -0.309420 0.438734 -1.731025 -0.233053 0.150694 2.214514 ......</span><br></pre></td></tr></table></figure>
<p>即每个词一行， 后面是400个数字， 即将每一个词变为一个400维的向量。(size is the dimensionality of the feature vectors.)<br>因此，Word2Vec是将每个语义词，转化成一个n维向量。根据这些向量，可以进行同义词操作;以及输入一个词，定位到其向量;预测词与句法的关联性等。</p>
<h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p><a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> is an unsupervised learning algorithm for obtaining vector representations (embeddings) for words. GloVe vectors serve the same purpose as word2vec but have different vector representations due to being trained on co-occurrence statistics.<br><a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a></p>
<h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><p>The position embedding is a vector concatenated with the word embedding. It represent the relative distance between the current word and the entity word.</p>
<p>The position embedding of  <strong>&lt;convolutional sequence to sequence learning&gt;</strong>:</p>
<p>For each word to translate, the input contains both the word itself and its position in the input chain (say, 0, 1, …m). Now, encoding such a data with simply having a cell with value pos (in 0..m) would not perform very well (for the same reason we use one-hot vectors to encode words). So, basically, the position will be encoded in a number of input cells, with one-hot representation (or similar, I might think of a binary representation of the position being used).</p>
<p>Then, an embedding layer will be used (just as it is used for word encodings) to transform this <em>sparse and discrete representation</em> into a <em>continuous</em> one.</p>
<p>The representation used in the paper chose to have the same dimension for the word embedding and the position embedding and to simply sum up the two.<br>中文解释：在输入信息中加入位置向量P=（p1,p2,….），把位置向量与词向量W=（w1，w2,…..）求和构成向量E=(w1+p1,w2+p2)，做为网络输入，使由CNN构成的Encoder和Decoder也具备了RNN捕捉输入Sequence中词的位置信息的功能。</p>
<h3 id="Dropout-in-Embedding"><a href="#Dropout-in-Embedding" class="headerlink" title="Dropout in Embedding"></a>Dropout in Embedding</h3><p>Applying dropout to the input of an embedding layer by selectively dropping certain ids is an effective method for preventing overfitting.<br>For example, if the embedding is a word2vec embedding, this method of dropout might drop the word “the” from the entire input sequence. In this case, the input “the dog and the cat” would become “– dog and – cat”. The input would never become “– dog and the cat”. This is useful to prevent the model from depending on certain words.</p>
<p>References:<br><a href="http://www.jeyzhang.com/tensorflow-learning-notes-3.html" target="_blank" rel="noopener">词向量(Word Embedding) &amp; Word2Vec详细</a><br><a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">Tensorflow-Embedding &amp; Word2Vec</a><br><a href="http://www.wildml.com/deep-learning-glossary/#glove" target="_blank" rel="noopener">GloVe简介</a><br><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe详细</a></p>
<h2 id="Encoder-Decoder-Model-RNN-and-Attention-mechanism"><a href="#Encoder-Decoder-Model-RNN-and-Attention-mechanism" class="headerlink" title="Encoder-Decoder Model, RNN and Attention mechanism"></a>Encoder-Decoder Model, RNN and Attention mechanism</h2><p>References:<br><a href="http://blog.csdn.net/u014595019/article/details/52826423" target="_blank" rel="noopener">Encoder-Decoder模型和Attention模型</a></p>
<h2 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h2><p><a href="https://weimin17.github.io/2017/09/Deep-Learning-Kernel/">My Blog - Kernel</a></p>
<h2 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual Learning"></a>Residual Learning</h2><p>–&gt;<a href="https://weimin17.github.io/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/">[Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING</a></p>
<!--more-->
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/11/Reading-List-Visual-Question-and-Answering-Video-QA-Image-QA/" rel="prev" title="[Reading List] Visual Question and Answering (Video QA & Image QA)">
      <i class="fa fa-chevron-left"></i> [Reading List] Visual Question and Answering (Video QA & Image QA)
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/01/Learning-Notes-Math-First-order-Stochastic-Algorithms-for-Escaping-From-Saddle-Points-in-Almost-Linear-Time/" rel="next" title="[Learning Notes-Math] First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time">
      [Learning Notes-Math] First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">1.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding"><span class="nav-number">2.</span> <span class="nav-text">Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2vec-amp-GloVe"><span class="nav-number">2.1.</span> <span class="nav-text">Word2vec &amp; GloVe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Word2vec"><span class="nav-number">2.1.1.</span> <span class="nav-text">Word2vec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GloVe"><span class="nav-number">2.1.2.</span> <span class="nav-text">GloVe</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Position-Embedding"><span class="nav-number">2.2.</span> <span class="nav-text">Position Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-in-Embedding"><span class="nav-number">2.3.</span> <span class="nav-text">Dropout in Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder-Decoder-Model-RNN-and-Attention-mechanism"><span class="nav-number">3.</span> <span class="nav-text">Encoder-Decoder Model, RNN and Attention mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernel"><span class="nav-number">4.</span> <span class="nav-text">Kernel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Residual-Learning"><span class="nav-number">5.</span> <span class="nav-text">Residual Learning</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="weimin"
      src="/images/avatar_idea.jpeg">
  <p class="site-author-name" itemprop="name">weimin</p>
  <div class="site-description" itemprop="description">DL/ML Blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/weimin17" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;weimin17" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">weimin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id: ,
      el: 'wpac-rating',
      color: 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>












  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '54ab485ae91fc57bf25e',
      clientSecret: '98349a9bcb969cadebace85cb2bdfd2ec7ab5184',
      repo: 'weimin17.github.io',
      owner: 'weimin17',
      admin: ['weimin17'],
      id: 'ddf8c2b18366e55efe5bb6aaae0a65dc',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
