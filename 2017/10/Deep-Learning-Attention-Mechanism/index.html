<!DOCTYPE html>
<html lang="en">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="weimin" />



<meta name="description" content="首先要知道什么是attention。這裏兩篇博客，一篇宏觀介紹Attention in Long Short-Term Memory Recurrent Neural Networks，一篇從較細角度介紹Attention and Memory in Deep Learning and NLP。">
<meta property="og:type" content="article">
<meta property="og:title" content="[Deep Learning] Attention Mechanism">
<meta property="og:url" content="http://weimin17.github.io/2017/10/Deep-Learning-Attention-Mechanism/index.html">
<meta property="og:site_name" content="A DL&#x2F;ML Learner">
<meta property="og:description" content="首先要知道什么是attention。這裏兩篇博客，一篇宏觀介紹Attention in Long Short-Term Memory Recurrent Neural Networks，一篇從較細角度介紹Attention and Memory in Deep Learning and NLP。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png">
<meta property="og:image" content="http://weimin17.github.io/images/pasted-37.png">
<meta property="article:published_time" content="2017-10-24T00:29:00.000Z">
<meta property="article:modified_time" content="2017-11-24T01:34:08.000Z">
<meta property="article:author" content="weimin">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="A DL/ML Learner" type="application/atom+xml">



    <link rel="shortcut icon" href="/Coding.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-center-atom.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>[Deep Learning] Attention Mechanism | A DL/ML Learner</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar_idea.jpeg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">weimin</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Train like a beast.</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="true" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">Homepage</a></li>
                        
                            <li><a href="/archives/">Archives</a></li>
                        
                            <li><a href="/tags/">Tags</a></li>
                        
                            <li><a href="/about/">About Me</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:insistweiminlv@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="https://github.com/weimin17" target="_blank" rel="noopener" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Action-Recognition/" rel="tag">Action Recognition</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/" rel="tag">LSTM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Learning-Notes/" rel="tag">Learning Notes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neuron-Network/" rel="tag">Neuron Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neuron-Netwrok/" rel="tag">Neuron Netwrok</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optiminzation/" rel="tag">Optiminzation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Perceptron/" rel="tag">Perceptron</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reading-List/" rel="tag">Reading List</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Video-QA/" rel="tag">Video QA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VideoQA/" rel="tag">VideoQA</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://github.com/weimin17/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">Train like a monster.</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">weimin</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar_idea.jpeg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">weimin</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Train like a beast.</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">Homepage</a></li>
                
                    <li><a href="/archives/">Archives</a></li>
                
                    <li><a href="/tags/">Tags</a></li>
                
                    <li><a href="/about/">About Me</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:insistweiminlv@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/weimin17" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me"/>
</nav>
      <div class="body-wrap"><article id="post-Deep-Learning-Attention-Mechanism" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/10/Deep-Learning-Attention-Mechanism/" class="article-date">
      <time datetime="2017-10-24T00:29:00.000Z" itemprop="datePublished">2017-10-23</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      [Deep Learning] Attention Mechanism
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>首先要知道什么是attention。這裏兩篇博客，一篇宏觀介紹<a href="https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/" target="_blank" rel="noopener">Attention in Long Short-Term Memory Recurrent Neural Networks</a>，一篇從較細角度介紹<a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="noopener">Attention and Memory in Deep Learning and NLP</a>。</p>
<a id="more"></a>
<p><strong>Attention</strong> is usually combine with RNN, seq2seq, encoder-decoder, you can see my own blog <code>[Deep Learning] Seq2Seq</code> for developed information.</p>
<p>I combine some points in these two blogs:</p>
<h4 id="1-What-is-Attention"><a href="#1-What-is-Attention" class="headerlink" title="1. What is Attention"></a>1. <strong>What is Attention</strong></h4><p>Attention Mechanisms in Neural Networks are (very) loosely based on the visual attention mechanism found in humans. Human visual attention is well-studied and while there exist different models, all of them essentially come down to being able to  <font color=#DC143C><strong>focus on a certain region of an image with “high resolution” while perceiving the surrounding image in “low resolution”, and then adjusting the focal point over time.</strong></font></p>
<p>Attention in Neural Networks has a long history, particularly in image recognition. Examples include Learning to combine foveal glimpses with a third-order Boltzmann machine or Learning where to Attend with Deep Architectures for Image Tracking. But only recently have attention mechanisms made their way into recurrent neural networks architectures that are typically used in NLP (and increasingly also in vision). </p>
<p>Attention is the idea of freeing the encoder-decoder architecture from <em>the fixed-length internal representation</em> (Maybe to overcome the disadvantages of <em>long input sequences</em> is better). This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to <font color=#DC143C><strong>pay selective attention to these inputs and relate them to items in the output sequence</strong></font>. Put another way, each item in the output sequence is conditional on selective items in the input sequence. 最初attention是应用与RNN相关网络，后来也可以用到CNN相关网络（如文章[Convolutional Sequence to Sequence])</p>
<h4 id="2-Why-Attention"><a href="#2-Why-Attention" class="headerlink" title="2. Why Attention"></a>2. <strong>Why Attention</strong></h4><p>The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.</p>
<p>A limitation of the architecture is that it encodes the input sequence to a <strong>fixed length internal representation</strong>. This imposes limits on the length of input sequences that can be reasonably learned and results in <strong>worse performance</strong> for very <strong>long input sequences</strong>.</p>
<h4 id="3-Disadvantages"><a href="#3-Disadvantages" class="headerlink" title="3. Disadvantages"></a>3. <strong>Disadvantages</strong></h4><p>This increases the computational burden of the model, but results in a more targeted and better-performing model.</p>
<h4 id="4-Attention-Examples"><a href="#4-Attention-Examples" class="headerlink" title="4. Attention Examples"></a>4. <strong>Attention Examples</strong></h4><p>Blog: <a href="https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/" target="_blank" rel="noopener">Part5 Examples of Attention in Sequence Prediction</a><br>Several Tensorflow Implementations: <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py" target="_blank" rel="noopener">Tensorflow attention wrapper</a></p>
<p>In this part, I introduce several attention mechanisms. Those of which has a tensorflow implementations, I name them as “- tensorflow implementation”.</p>
<h5 id="Soft-Attention"><a href="#Soft-Attention" class="headerlink" title="Soft Attention"></a><strong>Soft Attention</strong></h5><p><a href="https://arxiv.org/abs/1511.04119" target="_blank" rel="noopener">Action Recognition using Visual Attention</a></p>
<p>######<strong>Attention in Translation - tensorflow implementation</strong></p>
<p> <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a><br>With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far.  So, in languages that are pretty well aligned (like English and German) the decoder would probably choose to attend to things sequentially. Attending to the first word when producing the first English word, and so on. That’s what was done in <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a> and look as follows:</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png" alt="@Neural Machine Translation by Jointly Learning to Align and Translate|center|300*300"></p>
<p>Here, The <code>y&#39;s</code> are our translated words produced by the decoder, and the <code>x&#39;s</code> are our source sentence words. The above illustration uses a bidirectional recurrent network, but that’s not important and you can just ignore the inverse direction. The important part is that each decoder output word y_t now depends on <strong>a weighted combination of all the input states</strong>, not just the last state. The a‘s are weights that define in how much of each input state should be considered for each output. So, if $a_{3,2}$ is a large number, this would mean that the decoder pays a lot of attention to the second state in the source sentence while producing the third word of the target sentence. The <code>a&#39;s</code> are typically normalized to sum to 1 (so they are a distribution over the input states).</p>
<p>A big advantage of attention is that it gives us the ability to interpret and visualize what the model is doing. For example, by visualizing the attention weight matrix a when a sentence is translated, we can understand how the model is translating:</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png" alt="@Visualization| center| 300*300"></p>
<h6 id="Attention-in-Translation"><a href="#Attention-in-Translation" class="headerlink" title="Attention in Translation"></a><strong>Attention in Translation</strong></h6><p><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></p>
<p><img src="/images/pasted-37.png" alt="upload successful"></p>
<h5 id="Hard-Attention"><a href="#Hard-Attention" class="headerlink" title="Hard Attention"></a><strong>Hard Attention</strong></h5><h6 id="BahdanauMonotonicAttention-tensorflow-implementation"><a href="#BahdanauMonotonicAttention-tensorflow-implementation" class="headerlink" title="BahdanauMonotonicAttention - tensorflow implementation"></a><strong>BahdanauMonotonicAttention - tensorflow implementation</strong></h6><p><a href="https://arxiv.org/abs/1704.00784" target="_blank" rel="noopener">Online and Linear-Time Attention by Enforcing Monotonic Alignments</a><br><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauMonotonicAttention" target="_blank" rel="noopener">Tensorflow Doc</a></p>
<p>Short comings of <code>Soft Attentin</code>:<br>However, the fact that soft attention mechanisms perform a pass over the<br>entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity.</p>
<p>The focus of this paper is to propose an alternative attention mechanism which has linear-time complexity and can be used in online settings.</p>
<p>We believe our framework presents a promising environment for future<br>work on online and linear-time sequence-to-sequence models. </p>
<h5 id="LuongMonotonicAttention-tensorflow-implementation"><a href="#LuongMonotonicAttention-tensorflow-implementation" class="headerlink" title="LuongMonotonicAttention - tensorflow implementation"></a><strong>LuongMonotonicAttention - tensorflow implementation</strong></h5><p>  Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,<br>  “Online and Linear-Time Attention by Enforcing Monotonic Alignments.”<br>  ICML 2017.  <a href="https://arxiv.org/abs/1704.00784" target="_blank" rel="noopener">https://arxiv.org/abs/1704.00784</a></p>
<h5 id="Examples-of-Attention-in-Image-Descriptions"><a href="#Examples-of-Attention-in-Image-Descriptions" class="headerlink" title="Examples of Attention in Image Descriptions"></a><strong>Examples of Attention in Image Descriptions</strong></h5><p>Paper:<br><a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></p>
<h4 id="5-Attention-Fuzzy-Memory"><a href="#5-Attention-Fuzzy-Memory" class="headerlink" title="5.  Attention = (Fuzzy) Memory?"></a>5.  <strong>Attention = (Fuzzy) Memory?</strong></h4><p>The basic problem that the attention mechanism solves is that it allows the network to refer back to the input sequence, instead of forcing it to encode all information into one fixed-length vector. As I mentioned above, I think that attention is somewhat of a misnomer. Interpreted another way, the attention mechanism is simply giving the network access to its internal memory, which is the hidden state of the encoder. In this interpretation, instead of choosing what to “attend” to, the network chooses what to retrieve from memory. Unlike typical memory, the memory access mechanism here is soft, which means that the network retrieves a weighted combination of all memory locations, not a value from a single discrete location. Making the memory access soft has the benefit that we can easily train the network end-to-end using backpropagation (though there have been non-fuzzy approaches where the gradients are calculated using sampling methods instead of backpropagation).</p>
<p>Memory Mechanisms themselves have a much longer history. The hidden state of a standard Recurrent Neural Network is itself a type of internal memory. RNNs suffer from the vanishing gradient problem that prevents them from learning long-range dependencies. LSTMs improved upon this by using a gating mechanism that allows for explicit memory deletes and updates.</p>
<p>The trend towards more complex memory structures is now continuing. End-to-End Memory Networks allow the network to read same input sequence multiple times before making an output, updating the memory contents at each step. For example, answering a question by making multiple reasoning steps over an input story. However, when the networks parameter weights are tied in a certain way, the memory mechanism inEnd-to-End Memory Networks identical to the attention mechanism presented here, only that it makes multiple hops over the memory (because it tries to integrate information from multiple sentences).</p>
<p>Neural Turing Machines use a similar form of memory mechanism, but with a more sophisticated type of addressing that using both content-based (like here) and location-based addressing, allowing the network to learn addressing pattern to execute simple computer programs, like sorting algorithms.</p>
<p>It’s likely that in the future we will see a clearer distinction between memory and attention mechanisms, perhaps along the lines of Reinforcement Learning Neural Turing Machines, which try to learn access patterns to deal with external interfaces.</p>
<!--more-->
      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>Title:</span><a href="/2017/10/Deep-Learning-Attention-Mechanism/">[Deep Learning] Attention Mechanism</a></p>
        <p><span>Author:</span><a href="/" title="Back to Homepage">weimin</a></p>
        <p><span>Created:</span>2017-10-23, 20:29:00</p>
        <p><span>Updated:</span>2017-11-23, 20:34:08</p>
        <p>
            <span>Full URL:</span><a class="post-url" href="/2017/10/Deep-Learning-Attention-Mechanism/" title="[Deep Learning] Attention Mechanism">http://weimin17.github.io/2017/10/Deep-Learning-Attention-Mechanism/</a>
            <span class="copy-path" data-clipboard-text="From http://weimin17.github.io/2017/10/Deep-Learning-Attention-Mechanism/　　By weimin" title="Copy Article&#39;s Link &amp; Author"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>License:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"CC BY-NC-SA 4.0"</a> Keep Link &amp; Author if Distribute.
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2017/10/Deep-Learning-Gated-Linear-Unit/">
                    [Deep Learning] Gated Linear Unit
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2017/10/Deep-Learning-Extensions-of-Recurrent-Neural-Networks/">
                    [Deep Learning] Extensions of Recurrent Neural Networks
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
            <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-What-is-Attention"><span class="toc-text">1. What is Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Why-Attention"><span class="toc-text">2. Why Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Disadvantages"><span class="toc-text">3. Disadvantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Attention-Examples"><span class="toc-text">4. Attention Examples</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Soft-Attention"><span class="toc-text">Soft Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Attention-in-Translation"><span class="toc-text">Attention in Translation</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hard-Attention"><span class="toc-text">Hard Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#BahdanauMonotonicAttention-tensorflow-implementation"><span class="toc-text">BahdanauMonotonicAttention - tensorflow implementation</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#LuongMonotonicAttention-tensorflow-implementation"><span class="toc-text">LuongMonotonicAttention - tensorflow implementation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Examples-of-Attention-in-Image-Descriptions"><span class="toc-text">Examples of Attention in Image Descriptions</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Attention-Fuzzy-Memory"><span class="toc-text">5.  Attention &#x3D; (Fuzzy) Memory?</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="Hide"  title="Show or Hide Table of Contents">

    <script>
        yiliaConfig.toc = ["Hide", "Show", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"[Deep Learning] Attention Mechanism　| A DL/ML Learner　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    
        <section id="comments">
    <style> aside.comment-bar { margin: auto 30px; }</style>
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function(){
            this.page.url = 'http://weimin17.github.io/2017/10/Deep-Learning-Attention-Mechanism/';
            this.page.identifier = '2017/10/Deep-Learning-Attention-Mechanism/';
        };
        var loadComment = function(){
            var d = document, s = d.createElement('script');
            s.src = '//weimin17.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        }
    </script>
    
    <script> loadComment(); </script>

</section>


    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2017/10/Deep-Learning-Gated-Linear-Unit/" title="Pre: [Deep Learning] Gated Linear Unit">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="Mini Archives"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2017/10/Deep-Learning-Extensions-of-Recurrent-Neural-Networks/" title="Next: [Deep Learning] Extensions of Recurrent Neural Networks">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/Brief-Introductions-of-Selected-Research-Weimin-Lyu-Lv/">Brief Introductions of Selected Research | Littlekoala</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/LearningNotes-Deep-Visual-Semantic-Alignments-for-Generating-Image-Descriptions-2015CVPR/">[LearningNotes]Deep Visual-Semantic Alignments for Generating Image Descriptions(2015CVPR)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/LearningNotew-LearningNotes-Explainable-Prediction-of-Medical-Codes-from-Clinical-Text/">[LearningNotes][LearningNotes]Explainable Prediction of Medical Codes from Clinical Text</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/LearningNotes-TieNet-Text-Image-Embedding-Network-for-Common-Thorax-Disease-Classification-and-Reporting-in-Chest-X-rays-2018CVPR/">[LearningNotes]TieNet Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays(2018CVPR)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/Learning-Notes-Math-Scheduling-with-Predictable-Link-Reliability-for-Wireless-Networked-Control/">[Learning Notes-Math] Scheduling with Predictable Link Reliability for Wireless Networked Control</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/Learning-Notes-Velocity-Optimization-of-Pure-Electric-Vehicles-with-Traffic-Dynamics-Consideration/">[Learning Notes] Velocity Optimization of Pure Electric Vehicles with Traffic Dynamics Consideration</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/Learning-Notes-Math-First-order-Stochastic-Algorithms-for-Escaping-From-Saddle-Points-in-Almost-Linear-Time/">[Learning Notes-Math] First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/Deep-Learning-Warehouse/">Deep Learning Warehouse</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/Reading-List-Visual-Question-and-Answering-Video-QA-Image-QA/">[Reading List] Visual Question and Answering (Video QA & Image QA)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/Learning-Notes-Sequence-to-sequence-Learning-with-Neural-Networks/">[Learning Notes] Sequence to sequence Learning with Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/Learning-Notes-Convolutional-LSTM-Network-A-Machine-Learning-Approach-for-Precipitation-Nowcasting/">[Learning Notes] Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/Un-Learning-Notes-End-to-end-Concept-Word-Detection-for-Video-Captioning-Retrieval-and-Question-Answeringtitled/">Un[Learning Notes] End-to-end Concept Word Detection for Video Captioning, Retrieval and Question Answeringtitled</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Embedding/">[Deep Learning] Embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Residual-Connect/">[Deep Learning] Residual Connect</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Gated-Linear-Unit/">[Deep Learning] Gated Linear Unit</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Attention-Mechanism/">[Deep Learning] Attention Mechanism</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Extensions-of-Recurrent-Neural-Networks/">[Deep Learning] Extensions of Recurrent Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Seq2Seq/">[Deep Learning] Seq2Seq</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Convolutional-Neural-Networks-CNN/">[Deep Learning] Convolutional Neural Networks - CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Long-Short-Term-Memory-LSTM-Networks/">[Deep Learning] Long Short Term Memory - (LSTM) Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/Deep-Learning-Encoder-Decoder-Model/">[Deep Learning] Encoder-Decoder Model</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Learning-Notes-Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset/">[Learning Notes] Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset </a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Deep-Learning-Kernel/">[Deep Learning] Kernel</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Learning-Notes-CONVOLUTIONAL-SEQUENCE-TO-SEQUENCE-LEARNING/">[Learning Notes]CONVOLUTIONAL SEQUENCE TO SEQUENCE LEARNING</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Deep-Learning-Recurrent-Neural-Networks-RNN/">[Deep Learning] Recurrent Neural Networks - RNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Implement-The-Perceptron-Algorithm-in-Python-version2/">Implement The Perceptron Algorithm in Python-version2</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Numpy%E5%BA%94%E7%94%A8/">Numpy应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/%E7%AC%94%E8%AE%B0A-New-Representation-of-Skeleton-Sequences-for-3D-Action-Recognition/">[Learning notes] A New Representation of Skeleton Sequences for 3D Action Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Implement-The-Perceptron-Algorithm-in-Python-version1/">Implement The Perceptron Algorithm in Python-version1</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Learning-Notes-A-New-Representation-of-Skeleton-Sequences-for-3D-Action-Recognition/">[Learning Notes]A New Representation of Skeleton Sequences for 3D Action Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Mac-Using-Github-Hexo-to-build-personal-blog-2/">Mac: Using Github+Hexo to build personal blog(2)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/Mac-Github-Hexo-to-build-personal-blog/">Mac: Using Github+Hexo to build personal blog(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/hello-world/">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2017-2020 weimin
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
    
<script src="/js/GithubRepoWidget.js"></script>


<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
             github: ".github-widget a", 
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>